{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\pc daniel\\appdata\\roaming\\python\\python312\\site-packages (2.0.30)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\pc daniel\\appdata\\roaming\\python\\python312\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pc daniel\\appdata\\roaming\\python\\python312\\site-packages (from sqlalchemy) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# Define la configuración de la base de datos\n",
    "database_config = {\n",
    "    'server': r'DESKTOP-S9274BN\\SQLEXPRESS',\n",
    "    'database': 'nba_henry',\n",
    "    # 'username': 'your_username',\n",
    "    # 'password': 'your_password'\n",
    "}\n",
    "\n",
    "# Crear la cadena de conexión\n",
    "connection_string = f\"mssql+pyodbc://@{database_config['server']}/{database_config['database']}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "# Crear el motor de conexión\n",
    "engine = create_engine(connection_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODIGO FALLIDO CON CONSULTAS DINAMICAS\n",
    "class DataHandler(FileSystemEventHandler):\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        print(\"DataHandler initialized.\")\n",
    "\n",
    "    def on_created(self, event):\n",
    "        print(f\"Event detected: {event}\")\n",
    "        if event.is_directory:\n",
    "            print(\"The created event is a directory. Ignoring.\")\n",
    "            return None\n",
    "        elif event.src_path.endswith(\".csv\"):\n",
    "            print(f\"CSV file detected: {event.src_path}\")\n",
    "            self.process_new_file(event.src_path)\n",
    "\n",
    "    def process_new_file(self, file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Read the new CSV file and drop the 'Unnamed: 0' column if it exists\n",
    "        new_data = pd.read_csv(file_path)\n",
    "        if 'Unnamed: 0' in new_data.columns:\n",
    "            new_data = new_data.drop(columns=['Unnamed: 0'])\n",
    "        print(f\"New data read from {file_path}:\\n{new_data.head()}\")\n",
    "\n",
    "        # Load existing data from the database\n",
    "        existing_data = self.load_existing_data()\n",
    "        print(f\"Existing data loaded from database:\\n{existing_data.head()}\")\n",
    "\n",
    "        # Identify new or modified rows\n",
    "        changes = self.get_changes(existing_data, new_data)\n",
    "        print(f\"Changes identified:\\n{changes.head()}\")\n",
    "\n",
    "        # Process and insert the changes into SQL Server\n",
    "        self.insert_data_into_sql(changes)\n",
    "\n",
    "    def load_existing_data(self):\n",
    "        query = \"SELECT * FROM team\"\n",
    "        with self.engine.connect() as connection:\n",
    "            existing_data = pd.read_sql(query, connection)\n",
    "        return existing_data\n",
    "\n",
    "    def get_changes(self, existing_data, new_data):\n",
    "        merged_data = new_data.merge(existing_data, on='id', how='left', suffixes=('', '_existing'))\n",
    "\n",
    "        # Identify rows that are new or modified\n",
    "        changed_data = merged_data[merged_data.apply(\n",
    "            lambda row: any([row[col] != row.get(f\"{col}_existing\", None) for col in new_data.columns if col != 'id']), axis=1)]\n",
    "    \n",
    "        # Drop the '_existing' columns\n",
    "        changed_data = changed_data[new_data.columns]\n",
    "        return changed_data\n",
    "\n",
    "    def insert_data_into_sql(self, data):\n",
    "        if data.empty:\n",
    "            print(\"No new or modified rows to insert.\")\n",
    "            return\n",
    "        \n",
    "        with self.engine.connect() as connection:\n",
    "            # Obtener los nombres de las columnas\n",
    "            columns = data.columns.tolist()\n",
    "\n",
    "            for index, row in data.iterrows():\n",
    "                unique_key = row['id']\n",
    "\n",
    "                # Verificar si existe el registro\n",
    "                select_query = text(\"SELECT COUNT(1) FROM team WHERE id = :id\")\n",
    "                exists = connection.execute(select_query, {'id': unique_key}).scalar()\n",
    "\n",
    "                if exists:\n",
    "                    # Construir la consulta UPDATE dinámicamente\n",
    "                    update_columns = \", \".join([f\"{col} = :{col}\" for col in columns if col != 'id'])\n",
    "                    update_values = {col: row[col] for col in columns if col != 'id'}\n",
    "                    update_values['id'] = unique_key\n",
    "                    update_query = text(f\"UPDATE team SET {update_columns} WHERE id = :id\")\n",
    "                    connection.execute(update_query, update_values)\n",
    "                    print('Update')\n",
    "                    print(f\"Row with unique key {unique_key} updated.\")\n",
    "                else:\n",
    "                    # Construir la consulta INSERT dinámicamente\n",
    "                    insert_columns = \", \".join(columns)\n",
    "                    insert_placeholders = \", \".join([f\":{col}\" for col in columns])\n",
    "                    insert_values = {col: row[col] for col in columns}\n",
    "                    insert_query = text(f\"INSERT INTO team ({insert_columns}) VALUES ({insert_placeholders})\")\n",
    "                    print(insert_query)\n",
    "                    connection.execute(insert_query, insert_values)\n",
    "                    print('Insert')\n",
    "                    print(connection_string)\n",
    "                    print(f\"Row with unique key {unique_key} inserted.\")\n",
    "\n",
    "        print(\"Data inserted/updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODIGO SIMULACRO EXITOSO\n",
    "class DataHandler(FileSystemEventHandler):\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        print(\"DataHandler initialized.\")\n",
    "\n",
    "    def on_created(self, event):\n",
    "        print(f\"Event detected: {event}\")\n",
    "        if event.is_directory:\n",
    "            print(\"The created event is a directory. Ignoring.\")\n",
    "            return None\n",
    "        elif event.src_path.endswith(\".csv\"):\n",
    "            print(f\"CSV file detected: {event.src_path}\")\n",
    "            self.process_new_file(event.src_path)\n",
    "\n",
    "    def process_new_file(self, file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Read the new CSV file and drop the 'Unnamed: 0' column if it exists\n",
    "        new_data = pd.read_csv(file_path)\n",
    "        if 'Unnamed: 0' in new_data.columns:\n",
    "            new_data = new_data.drop(columns=['Unnamed: 0'])\n",
    "        print(f\"New data read from {file_path}:\\n{new_data.head()}\")\n",
    "\n",
    "        # Load existing data from the database\n",
    "        existing_data = self.load_existing_data()\n",
    "        print(f\"Existing data loaded from database:\\n{existing_data.head()}\")\n",
    "\n",
    "        # Identify new or modified rows\n",
    "        changes = self.get_changes(existing_data, new_data)\n",
    "        print(f\"Changes identified:\\n{changes.head()}\")\n",
    "\n",
    "        # Process and insert the changes into SQL Server\n",
    "        self.insert_data_into_sql(changes)\n",
    "\n",
    "    def load_existing_data(self):\n",
    "        query = \"SELECT * FROM team\"\n",
    "        with self.engine.connect() as connection:\n",
    "            existing_data = pd.read_sql(query, connection)\n",
    "        return existing_data\n",
    "\n",
    "    def get_changes(self, existing_data, new_data):\n",
    "        merged_data = new_data.merge(existing_data, on='id', how='left', suffixes=('', '_existing'))\n",
    "\n",
    "        # Identify rows that are new or modified\n",
    "        changed_data = merged_data[merged_data.apply(\n",
    "            lambda row: any([row[col] != row.get(f\"{col}_existing\", None) for col in new_data.columns if col != 'id']), axis=1)]\n",
    "    \n",
    "        # Drop the '_existing' columns\n",
    "        changed_data = changed_data[new_data.columns]\n",
    "        return changed_data\n",
    "\n",
    "    def insert_data_into_sql(self, data):\n",
    "        if data.empty:\n",
    "            print(\"No new or modified rows to insert.\")\n",
    "            return\n",
    "        \n",
    "        with self.engine.connect() as connection:\n",
    "            # Obtener los nombres de las columnas\n",
    "            data.to_sql('team', connection, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler(FileSystemEventHandler):\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        print(\"DataHandler initialized.\")\n",
    "\n",
    "    def on_created(self, event):\n",
    "        print(f\"Event detected: {event}\")\n",
    "        if event.is_directory:\n",
    "            print(\"The created event is a directory. Ignoring.\")\n",
    "            return None\n",
    "        elif event.src_path.endswith(\".csv\"):\n",
    "            print(f\"CSV file detected: {event.src_path}\")\n",
    "            self.process_new_file(event.src_path)\n",
    "\n",
    "    def process_new_file(self, file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Read the new CSV file and drop the 'Unnamed: 0' column if it exists\n",
    "        new_data = pd.read_csv(file_path)\n",
    "        if 'Unnamed: 0' in new_data.columns:\n",
    "            new_data = new_data.drop(columns=['Unnamed: 0'])\n",
    "        print(f\"New data read from {file_path}:\\n{new_data.head()}\")\n",
    "\n",
    "        # Load existing data from the database\n",
    "        existing_data = self.load_existing_data(filename)\n",
    "        print(f\"Existing data loaded from database:\\n{existing_data.head()}\")\n",
    "\n",
    "        # Identify new or modified rows\n",
    "        changes = self.get_changes(existing_data, new_data,filename)\n",
    "        print(f\"Changes identified:\\n{changes.head()}\")\n",
    "\n",
    "        # Process and insert the changes into SQL Server\n",
    "        self.insert_data_into_sql(changes,filename)\n",
    "\n",
    "    def load_existing_data(self,filename):\n",
    "        print(filename + \" load existing data\")\n",
    "      \n",
    "        filename_without_extension = filename.replace(\".csv\", \"\")\n",
    "        \n",
    "        query = f\"SELECT * FROM {filename_without_extension}\"\n",
    "        print(query)\n",
    "        \n",
    "        with self.engine.connect() as connection:\n",
    "            existing_data = pd.read_sql(query, connection)\n",
    "        return existing_data\n",
    "\n",
    "    def get_changes(self, existing_data, new_data,filename):\n",
    "        print(filename + \" get changes\")\n",
    "        merged_data = new_data.merge(existing_data, on='id', how='left', suffixes=('', '_existing'))\n",
    "\n",
    "        # Identify rows that are new or modified\n",
    "        changed_data = merged_data[merged_data.apply(\n",
    "            lambda row: any([row[col] != row.get(f\"{col}_existing\", None) for col in new_data.columns if col != 'id']), axis=1)]\n",
    "    \n",
    "        # Drop the '_existing' columns\n",
    "        changed_data = changed_data[new_data.columns]\n",
    "        return changed_data\n",
    "\n",
    "    def insert_data_into_sql(self, data,filename):\n",
    "        filename_without_extension = filename.replace(\".csv\", \"\")\n",
    "        \n",
    "        if data.empty:\n",
    "            print(\"No new or modified rows to insert.\")\n",
    "            return\n",
    "        \n",
    "        with self.engine.connect() as connection:\n",
    "            # Obtener los nombres de las columnas\n",
    "            data.to_sql(filename_without_extension, connection, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to watch directory: data/watchdog_carpeta\n",
      "DataHandler initialized.\n",
      "Event detected: FileCreatedEvent(src_path='data/watchdog_carpeta\\\\game_info.csv', dest_path='', event_type='created', is_directory=False, is_synthetic=False)\n",
      "CSV file detected: data/watchdog_carpeta\\game_info.csv\n",
      "Processing file: data/watchdog_carpeta\\game_info.csv\n",
      "New data read from data/watchdog_carpeta\\game_info.csv:\n",
      "    id   game_date  attendance\n",
      "0  101  2023-06-21     15000.0\n",
      "1  102  2023-06-22     20000.0\n",
      "2  103  2023-06-23     18000.0\n",
      "3  104  2023-06-24     22000.0\n",
      "game_info.csv load existing data\n",
      "SELECT * FROM game_info\n",
      "Existing data loaded from database:\n",
      "         id   game_date  attendance\n",
      "0  11300001  2013-10-05       12191\n",
      "1  11300002  2013-10-05       15273\n",
      "2  11300003  2013-10-05       15049\n",
      "3  11300005  2013-10-06       13538\n",
      "4  11300006  2013-10-06       16722\n",
      "game_info.csv get changes\n",
      "Changes identified:\n",
      "    id   game_date  attendance\n",
      "0  101  2023-06-21     15000.0\n",
      "1  102  2023-06-22     20000.0\n",
      "2  103  2023-06-23     18000.0\n",
      "3  104  2023-06-24     22000.0\n"
     ]
    }
   ],
   "source": [
    "# Define la función principal para iniciar el observador\n",
    "if __name__ == \"__main__\":\n",
    "    path_to_watch = 'data/watchdog_carpeta'  # Cambia esto a tu directorio real\n",
    "    print(f\"Starting to watch directory: {path_to_watch}\")\n",
    "    event_handler = DataHandler(engine)\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path=path_to_watch, recursive=False)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)  # Mantén el script en ejecución\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
